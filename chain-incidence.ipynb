{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Distance\n",
    "\n",
    "**Requirements**\n",
    "In order to run this notebook you will need:\n",
    "* `yelp`, `pandas`, `folium`, `geojson`, `numpy`, `requests`, `geopy`, and `bokeh` libraries (all available via `pip`).\n",
    "* A [Yelp! API key](https://www.yelp.com/developers/manage_api_keys), saved to a credentials file in the format recommended [here](https://github.com/Yelp/yelp-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from yelp.client import Client\n",
    "from yelp.oauth1_authenticator import Oauth1Authenticator\n",
    "from yelp.errors import BusinessUnavailable\n",
    "import os\n",
    "import json\n",
    "from pandas import DataFrame\n",
    "import folium\n",
    "import geojson\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "from geopy.distance import vincenty\n",
    "import bokeh.plotting as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def import_credentials(filename='yelp_credentials.json'):\n",
    "    \"\"\"\n",
    "    Finds the credentials file describing the token that's needed to access Yelp services.\n",
    "\n",
    "    :param filename -- The filename at which Yelp service credentials are stored. Defaults to\n",
    "    `yelp_credentials.json`.\n",
    "    \"\"\"\n",
    "    if filename in [f for f in os.listdir('.') if os.path.isfile(f)]:\n",
    "        data = json.load(open(filename))\n",
    "        return data\n",
    "    else:\n",
    "        raise IOError('This API requires Yelp credentials to work. Did you forget to define them?')\n",
    "\n",
    "def fetch_businesses(name, area='New York', manual_override=0):\n",
    "    area = area.lower().replace(' ', '-')\n",
    "    name = name.lower().replace(' ', '-')\n",
    "    \"\"\"\n",
    "    Fetches all yelp.obj.business_response.BusinessResponse objects for incidences of the given chain in Manhattan.\n",
    "    Constructs Yelp business ids for incidences of the chain in the area, then queries Yelp to check if they\n",
    "    exist.\n",
    "    IDs are constructed name-location-number, so we just have to check numbers in ascending order until it breaks.\n",
    "    e.g. http://www.yelp.com/biz/gregorys-coffee-new-york-18 is good.\n",
    "         http://www.yelp.com/biz/gregorys-coffee-new-york-200 is not.\n",
    "    Then we do reverse GIS searches using the business ID through the Yelp API and extract coordinates from the results.\n",
    "    Some technical notes:\n",
    "    1.  The first incidence of any store in the area is reported without any numeral.\n",
    "        e.g. \"dunkin-donuts-new-york\", not \"dunkin-donuts-new-york-1\".\n",
    "        Numbers pick up from there: the next shitty hole in the wall will be \"dunkin-donuts-new-york-2\".\n",
    "    2.  Yelp IDs are unique and are not reassigned when a location is closed.\n",
    "        Thus we need to check for and exclude closed locations when munging the data.\n",
    "    3.  Places with a single instance in Manhattan sometimes have a \"name-place-2\" that redirects to their only location.\n",
    "        At least this seems to be the case with Bibble & Sip...\n",
    "        This is checked and corrected for further down the line, by the fetch_businesses() method.\n",
    "    4.  Sometimes IDs are given to locations that don't actually really exist.\n",
    "        e.g. the best-buy-3 id points to a non-existant storefront.\n",
    "        But best-buy-4, best-buy-5, and so on actually exist!\n",
    "        Yelp acknowledges this fact, but still returns a BusinessUnavaialable error when queries.\n",
    "        This method sends a web request and checks the response and terminates on a 404, which has proven to be a reliable\n",
    "        way of circumnavigating this issue.\n",
    "    5.  In case the above doesn't work...\n",
    "        The manual_override parameter forces the fetcher to keep moving past this error.\n",
    "        For debugging purposes, this method prints a URL for the purposes of manually checking breakpoints.\n",
    "        That way you can incrementally run fetch() and then comb over trouble spots you find by moving up manual_override.\n",
    "        If you hit that URL and you get either a valid ID or an invalid but existing ID, you need to bump up manual_override\n",
    "        to correct it and rerun the fetch.\n",
    "        If you hit that URL and you get a 404 page then you're done!\n",
    "        e.x. In the Best Buy case both best-buy-3 and best-buy-10 are phantoms.\n",
    "        But once we set manual_override=10 we're good, and get all of the actual storefronts.\n",
    "    \"\"\"\n",
    "    i = 2\n",
    "    # Run the first one through by hand.\n",
    "    try:\n",
    "        responses = [client.get_business(\"{0}-{1}\".format(name, area))]\n",
    "    # This can happen, and did, in the Dunkin' Donuts case.\n",
    "    except BusinessUnavailable:\n",
    "        responses = []\n",
    "        pass\n",
    "    # The rest are handled by a loop.\n",
    "    while True:\n",
    "        bus_id = \"{0}-{1}-{2}\".format(name, area, i)\n",
    "        try:\n",
    "            response = client.get_business(bus_id)\n",
    "        except BusinessUnavailable:\n",
    "            # We manually check trouble spots.\n",
    "            # But see the TODO.\n",
    "            if requests.get('http://www.yelp.com/biz/' + bus_id).status_code != requests.codes.ok:\n",
    "                break\n",
    "            else:\n",
    "                # Increment the counter but don't include the troubled ID.\n",
    "                i += 1\n",
    "                continue\n",
    "        responses += [response]\n",
    "        i += 1\n",
    "    print(\"Ended `fetch_businesses()` on:\", \"http://www.yelp.com/biz/\" + bus_id)\n",
    "    return responses        \n",
    "\n",
    "\n",
    "def frame(responses):\n",
    "    \"\"\"\n",
    "    Given a list of yelp.obj.business_response.BusinessResponse objects like the one returns by fetch_businesses(),\n",
    "    builds a coordinate-logging DataFrame out of them.\n",
    "    \"\"\"\n",
    "    latitudes = [response.business.location.coordinate.latitude for response in responses]\n",
    "    longitudes = [response.business.location.coordinate.longitude for response in responses]\n",
    "    df = DataFrame({'latitude': latitudes, 'longitude': longitudes})\n",
    "    df.index.name=responses[0].business.name\n",
    "    return df\n",
    "\n",
    "\n",
    "def map_coordinates(df):\n",
    "    \"\"\"\n",
    "    Returns a folium map of all of the coordinates stored in a coordinate DataFrame, like the one returned by frame().\n",
    "    \"\"\"\n",
    "    ret = folium.Map(location=[40.753889, -73.983611], zoom_start=11)\n",
    "    for row in df.iterrows():\n",
    "        ret.simple_marker([row[1]['latitude'], row[1]['longitude']])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def load_geojson(filename=\"manhattan.geojson\"):\n",
    "    \"\"\"\n",
    "    Returns a geojson object for the given file.\n",
    "    \n",
    "    TODO: Add typing logic to distinguish between \"Feature\" and \"FeatureCollection\" objects.\n",
    "    Parsing these is not exactly equivalent. The latter consists of a list of features.\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        dat = f.read()\n",
    "        obj = geojson.loads(dat)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def load_coordinates(name):\n",
    "    \"\"\"\n",
    "    Loads Manhattan.\n",
    "    What else?\n",
    "    Are you surprised?\n",
    "    \"\"\"\n",
    "    # Encode according to our storage scheme.\n",
    "    filename = name.lower().replace(' ', '_') + '.geojson'\n",
    "    obj = load_geojson(filename)\n",
    "    if obj['type'] == 'FeatureCollection':\n",
    "        ret = []\n",
    "        for feature in obj['features']:\n",
    "            ret += list(geojson.utils.coords(feature))\n",
    "        # return ret\n",
    "    elif obj['type'] == 'Feature':\n",
    "        ret = list(geojson.utils.coords(obj))\n",
    "    # GeoJSON stores coordinates [Longitude, Latitude] -- the \"modern\" format.\n",
    "    # For historical reasons, coordinates are usually represented in the format [Latitude, Longitude].\n",
    "    # And this is indeed the format that the rest of the libraries used for this project expect.\n",
    "    # So we need to swap the two: [Longitude, Latitude] -> [Latitude, Longitude]\n",
    "    ret = [(coord[1], coord[0]) for coord in ret]\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Borrowed from: http://www.ariel.com.au/a/python-point-int-poly.html\n",
    "def point_inside_polygon(x, y, poly):\n",
    "    \"\"\"\n",
    "    Checks if a point is inside a polygon.\n",
    "    Used to validate points as being inside of Manahttan.\n",
    "    Borrowed from: http://www.ariel.com.au/a/python-point-int-poly.html\n",
    "    \n",
    "    The shapely library provides features for this and other things besides, but is too much to deal with at the moment.\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(poly)\n",
    "    inside = False\n",
    "\n",
    "    p1x,p1y = poly[0]\n",
    "    for i in range(n+1):\n",
    "        p2x,p2y = poly[i % n]\n",
    "        if y > min(p1y,p2y):\n",
    "            if y <= max(p1y,p2y):\n",
    "                if x <= max(p1x,p2x):\n",
    "                    if p1y != p2y:\n",
    "                        xints = (y-p1y)*(p2x-p1x)/(p2y-p1y)+p1x\n",
    "                    if p1x == p2x or x <= xints:\n",
    "                        inside = not inside\n",
    "        p1x,p1y = p2x,p2y\n",
    "\n",
    "    return inside\n",
    "\n",
    "\n",
    "def generate_sample_points(coordinate_list, n=1000):\n",
    "    \"\"\"\n",
    "    Generates n uniformly distributed sample points within the given coordinate list.\n",
    "    \n",
    "    When the geometry is sufficiently complex and the list of points large this query can take a while to process.\n",
    "    \"\"\"\n",
    "    lats, longs = list(map(lambda coords: coords[0], coordinate_list)), list(map(lambda coords: coords[1], coordinate_list))\n",
    "    max_lat = max(lats)\n",
    "    min_lat = min(lats)\n",
    "    max_long = max(longs)\n",
    "    min_long = min(longs)\n",
    "    ret = []\n",
    "    while True:\n",
    "        p_lat = random.uniform(min_lat, max_lat)\n",
    "        p_long = random.uniform(min_long, max_long)\n",
    "        if point_inside_polygon(p_lat, p_long, coordinate_list):\n",
    "            ret.append((p_lat, p_long))\n",
    "            if len(ret) > n:\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "    return ret\n",
    "\n",
    "\n",
    "def sample_points(search_location, n=10000):\n",
    "    \"\"\"\n",
    "    Given the name of the location being search, returns n uniformally distributed points within that location.\n",
    "    \n",
    "    Wraps the above.\n",
    "    \"\"\"\n",
    "    return generate_sample_points(load_coordinates(search_location), n)\n",
    "\n",
    "\n",
    "def get_minimum_distance(coordinate, coordinate_list):\n",
    "    \"\"\"\n",
    "    Naively calculates the minimum distance in the point cloud.\n",
    "    \"\"\"\n",
    "    best_coord = (0, 0)\n",
    "    best_distance = 1000\n",
    "    for candidate_coord in coordinate_list:\n",
    "        dist = vincenty(coordinate, candidate_coord).miles\n",
    "        if dist < best_distance:\n",
    "            best_coord = candidate_coord\n",
    "            best_distance = dist\n",
    "    return best_distance\n",
    "\n",
    "\n",
    "def average_distance(chain_name, search_location, point_cloud):\n",
    "    \"\"\"\n",
    "    This is the main function of this notebook!\n",
    "    Takes the name of the chain in question and the point cloud associated with the location\n",
    "    for which we are computing average distance.\n",
    "    Returns the average distance to that chain within that location.\n",
    "    We ask for a point cloud and not the name of the location because it's more efficient to precompute an extremely large,\n",
    "    essentially totally random point cloud, and then check against that, instead of recomputing it every round.\n",
    "    Output is in feet!\n",
    "    \"\"\"\n",
    "    # First load the coordinates corresponding to the search location..\n",
    "    location_coords = load_coordinates(search_location)\n",
    "    # Now generate a list of the chains' locations.\n",
    "    chain_df = frame(fetch_businesses(chain_name))\n",
    "    chain_coords = list(zip(chain_df['latitude'], chain_df['longitude']))\n",
    "    # Finally, get and average the minimum distances between the points in the point cloud and the chain locations.\n",
    "    distances = [get_minimum_distance(point, chain_coords) for point in point_cloud]\n",
    "    avg = sum(distances)/len(distances)\n",
    "    avg_in_feet = int(5280*avg)\n",
    "    return avg_in_feet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credentials = import_credentials()\n",
    "\n",
    "auth = Oauth1Authenticator(\n",
    "    consumer_key=credentials['consumer_key'],\n",
    "    consumer_secret=credentials['consumer_secret'],\n",
    "    token=credentials['token'],\n",
    "    token_secret=credentials['token_secret']\n",
    ")\n",
    "\n",
    "client = Client(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_buy = fetch_businesses('Best Buy')\n",
    "map_coordinates(frame(best_buy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gregorys = fetch_businesses('Gregorys Coffee')\n",
    "map_coordinates(frame(gregorys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "starbucks = fetch_businesses('Starbucks')\n",
    "map_coordinates(frame(starbucks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dunkindonuts = fetch_businesses('Dunkin Donuts')\n",
    "map_coordinates(frame(dunkindonuts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manhattan_point_cloud = sample_points(\"Manhattan\", n=2000)\n",
    "# Unfortunately the PiP algorithm does not handle seperate-part polygons, so I can't do non-linked shapes yet.\n",
    "# This becomes obvious when you plot the NYC point cloud.\n",
    "# new_york_city_point_cloud = sample_points(\"New York City\", n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.output_notebook(hide_banner=True)\n",
    "\n",
    "p = plt.figure(height=500,\n",
    "                width=960,\n",
    "                title=\"Manhattan Point Cloud\",\n",
    "                x_axis_label=\"Latitude\",\n",
    "                y_axis_label=\"Longitude\"\n",
    "               )\n",
    "\n",
    "p.scatter(\n",
    "    [coord[0] for coord in manhattan_point_cloud],\n",
    "    [coord[1] for coord in manhattan_point_cloud]\n",
    ")\n",
    "\n",
    "plt.output_file(filename=\"manhattan_point_cloud.html\")\n",
    "plt.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.output_notebook(hide_banner=True)\n",
    "\n",
    "# p = plt.figure(height=500,\n",
    "#                 width=960,\n",
    "#                 title=\"New York City Point Cloud\",\n",
    "#                 x_axis_label=\"Latitude\",\n",
    "#                 y_axis_label=\"Longitude\"\n",
    "#                )\n",
    "\n",
    "# p.scatter(\n",
    "#     [coord[1] for coord in new_york_city_point_cloud],\n",
    "#     [coord[0] for coord in new_york_city_point_cloud]\n",
    "# )\n",
    "\n",
    "# plt.output_file(filename=\"new_york_city_point_cloud.html\")\n",
    "# plt.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save data generated so far for later use\n",
    "map_coordinates(frame(dunkindonuts)).create_map(path='dunkin_donuts.html')\n",
    "map_coordinates(frame(starbucks)).create_map(path='starbucks.html')\n",
    "map_coordinates(frame(gregorys)).create_map(path='gregorys.html')\n",
    "map_coordinates(frame(best_buy)).create_map(path='best_buy.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_distance(\"Bibble and Sip\", \"Manhattan\", manhattan_point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unfortunately the PiP algorithm does not handle seperate-part polygons, so I can't do non-linked shapes yet.\n",
    "# This becomes obvious when you plot the NYC point cloud.\n",
    "# average_distance(\"Bibble and Sip\", \"New York City\", manhattan_point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "representative_chains = ['Cold Stone Creamery', 'Zara', 'Apple Store', 'Brooks Brothers', 'Whole Foods', 'Jamba Juice',\n",
    "                         'Forever 21', 'Target', 'Barnes And Noble', 'Chopt Creative Salad Company', 'H And M', 'IHOP',\n",
    "                         'Pizza Hut', 'Sephora', 'Taco Bell', 'Radio Shack', 'Chipotle', 'Gamestop', 'CVS', '7-Eleven',\n",
    "                        'GNC', 'Rite Aid', 'Baskin Robbins', 'McDonalds', 'Starbucks', 'Subway', 'Dunkin Donuts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distance_list = []\n",
    "for chain in representative_chains:\n",
    "    try:\n",
    "        dist = average_distance(chain, \"Manhattan\", manhattan_point_cloud)\n",
    "        distance_list.append([chain, dist])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add in Starbucks, Gregory's Coffee, and Best Buy.\n",
    "for chain in ['Gregorys Coffee', 'Best Buy', 'Starbucks']:\n",
    "    try:\n",
    "        dist = average_distance(chain, \"Manhattan\", manhattan_point_cloud)\n",
    "        distance_list.append([chain, dist])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save it and retrieve it for transfer (so you don't have to run all of those from scratch every time!)\n",
    "save_df = DataFrame(data=distance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data.json', 'w') as file:\n",
    "    file.write(save_df.to_json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
